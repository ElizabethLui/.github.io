# Elizabeth Lui
[(elizabethlui0616@gmail.com)]

# Bio

My name is Elizabeth Lui and I am currently working as a Researcher at the Computational Social Sciences and Legal Studies Lab, City University of Hong Kong. I hold a Master's Degree in Comparative Social Policy at University of Oxford, and a Bachelor's Degree in Government and Public Administration at the Chinese University of Hong Kong. In this coming fall, I will be pursuing a MS degree in Data Science at the University of Texas at Austin (part-time, remotely).
As an aspiring computational social scientist, I am proficient in programming languages such as Python and R, and familiar with methods including Natural Languague Processing, social network theory, machine learning and so on. You may see my projects under the Portfolio Section below.

# Portfolio

# [Project #1 - Natural Language Processing - Distributed Methods Using Python](https://github.com/ElizabethLui/nlp_dask/blob/efcbf7123f6b5a1432e8c35fc4990b5c13818eef/nlp_dask.ipynb)

This project aims to conduct distributed data cleaning and wrangling processing on a huge dataset (the final dataset is comprised of 100+ million tweets). This set of codes can be leveraged and scaled to any project that involves memory-intensive tasks.

I crawled this dataset from scratch using the new Twitter API v2 via the twarc package.


# [Project #2 - Granger Casaulity Test and Time Series Modelling Using R](https://elizabethlui.github.io/granger_election/granger_election.html)

This project builds a time-series model to test for Granger casuality between 2 variables: immigration population and electoral outcomes in Presidential Elections in USA from 1880 to 2000. It employs various methods including autocorrelation function (ACF), impulse response function (IRF) and optimal lags selection. 


# [Project #3 - Unstructured Data and Web Crawling Using Python ](https://github.com/ElizabethLui/web_crawling/blob/cd00dc6563ee646857882bc2b1569d6c15b83147/news.ipynb)

This project's aims are two fold. First, it cleans a messy, unstructured text file to find urls which link to an individual news article. Second, it loops over the list of urls (n=190) to extract useful information - i.e. title of news, publication date, as well as the tags used to classified the news.


# [Project #4 - Scraping Posts from Twitter and Tweets Analysis Using R](https://elizabethlui.github.io/hongkong_twitter/)

I scraped tweets in relation to the protests taking place during the anti-extradition movement in Hong Kong. This dataset was collected using the new Twitter API v2 and spans from June 2019 till January 2020. In this R script, I examined the trends of these tweets and looked at how foreign politicians (US & UK) responded to these pro-democracy protests on Twitter. 

